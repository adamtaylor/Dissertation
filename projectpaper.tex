\documentclass[12pt,a4paper]{article}
\usepackage{times}
\usepackage{durhampaper}
\usepackage{harvard}

\citationmode{abbr}
\bibliographystyle{agsm}

\title{Analysis of Web Application Vulnerability Detection Algorithms}
\author{David Budgen}
\student{A.J.C. Taylor}
\supervisor{W.W. Song}
\degree{BSc Software Engineering}

\date{}

\begin{document}

\maketitle

\begin{abstract}
These instructions give you guidelines for preparing the final paper.  DO NOT change any settings, such as margins and font sizes.  Just use this as a template and modify the contents into your final paper.  Do not cite references in the abstract.

The abstract must be a Structured Abstract with the headings {\bf Context/Background}, {\bf Aims}, {\bf Method}, {\bf Results}, and {\bf Conclusions}.  This section should not be longer than half of a page, and having no more than one or two sentences under each heading is advised.
\end{abstract}

\begin{keywords}
web application security, sql injection, cross site scripting, xss, brute force, fuzzing
\end{keywords}

\section{Introduction}
The aim of the project is twofold: firstly, the creation and evaluation of different attack algorithms to better detect security vulnerabilities and secondly to develop a tool, using the algorithms created, to aid in the automated detection of security vulnerabilities in websites and web applications.
The purpose of the tool is to help security researchers and website owners to identify areas of their websites and web applications that are vulnerable to particular attacks so that they can mitigate the problem(s).

It is prohibitively expensive to perform a manual vulnerability analysis of a web application, however, an automated tool can rapidly analyse a web application using several different attack methodologies much faster than a human.

The Open Web Application Security Project (OWASP) maintain an annual list of the most prevalent vulnerabilities found in web applications. [18].  The two vulnerabilities the algorithms and tool will be primarily addressing are Cross Site Scripting (XSS) and Injection Flaws, particularly SQL Injection (SQLi) flaws in our work.

Almost half of all the vulnerabilities found in web applications were either XSS vulnerabilities or Injection Flaws, which is why these two vulnerabilities are the primary focus of this research. 

XSS vulnerabilities "allows an attacker to manipulate parts of a Web page... By injecting malicious scripting code" [4] which according to a report by CERT allows an attacker to "run malicious code in a users browser with the privileges of a legitimate script originating from the legitimate web server". [19]

Vulnerabilities from XSS attacks include stealing the user's cookies and hijacking their session and/or account, connecting users to malicious servers of the attacker's choice or performing other malicious operations on the user's machine under the guise of the vulnerable site. [12]

SQL injection attacks revolve around the attacker "trying to exploit the data provider at the backend...leading to information disclosure". [4]

SQL injection attacks “allow attackers to create, read, update, or delete any arbitrary data available to the application. In the worst case scenario, these flaws allow an attacker to completely compromise the application and the underlying systems, even bypassing deeply nested firewalled environments.” [18] 

\section{Related Work}
There are a number of different approaches to vulnerability discovery and prevention.  Some of the discovery work involves static analysis of source code (white box) and some of the prevention work involves input filtering techniques that can be applied to web applications.
Much of the automated vulnerability discovery work has been done by commercial companies, however, there are some free and/or open source implementations.

\subsection{Static Analysis}
Static analysis is a white box technique used to analyse source code for vulnerabilities.  The main limitation of static analysis tools is that they have to be rewritten for every different programming language and/or framework a web application can be written in, and so far, most of the work seems to have focussed only on PHP.  They also struggle to analyse dynamically included content.

Wasserman and Su worked on static analysis to detect XSS vulnerabilities using a mixture of tainted information flow and  string analysis and were able to provide effective checking algorithms. [22]

Wasserman and Su also worked on a static analysis tool to detect SQLi vulnerabilities.  They constructed a grammar based algorithm which tracks the effects of string operations and checks query construction to see whether users can directly or indirectly influence the query construction.   They then check if the users’ input is sufficiently  isolated from the query construction that it avoids SQLi vulnerabilities. [23]

Jovanovic, Kruegel and  Kirda created an open source tool - Pixy - for static analysis of PHP applications to detect XSS vulnerabilities.  The tool used a “flow sensitive, interprocedural, and context sensitive analysis for PHP” to detect vulnerabilities. [8]  Pixy is hampered by its inability to scan included PHP files, leading to the authors admitting a number of false-positives being returned by the tool.  Also Pixy does not deal with the object-orientated features of PHP, instead they assume that object member variables are safe, which may not be the case.

Huang et al. created a “lattice-based static analysis algorithm, derived from type systems and typestate”.  They also created a tool - WebSSARI - to analyse PHP source code for XSS and SQLi vulnerabilities. [5]

\subsection{Input Filtering}
Input filtering research has been undertaken by developing intermediary proxies that can filter out data before it reaches, for example, a user or a database.

Liu et al. developed  a proxy-tool, called SQLProb, to prevent SQLi attacks.  SQLProb sits between the front-end web servers and the back-end databases and uses genetic algorithms to dynamically detect SQLi attacks and block them from reaching the databases. [11]

SQLProb works in a similar manner to a web application firewall (WAF) and is quite a useful additional layer of security for a web application.  There are a number of other open-source WAFs such as: mod_security for the Apache web server [14] and WebKnight for the IIS web server. [1]  There are also many commercial WAFs available.

Kirda et al. developed a proxy tool - Noxes - for end-users to sit between  web applications and the users’ browser to mitigate the effect of XSS vulnerabilities on end-users.  Noxes can be thought of as a personal web firewall for end-users. [10] Noxes is similar to the FireFox plugin NoScript [17] and suffers the same limitations in that it is extremely invasive to have to block or allow every single request from every new URI.  Noxes plays no role in discovering vulnerabilities in web applications, it is purely for protecting end users.

\subsection{Automated Black Box Vulnerability Discovery}
Automated black box vulnerability discovery tools look at a web application from a [malicious] user’s perspective, i.e. there is no source code analysis involved, and only look at the front-end of the web application.  Much of the research and developed tools have been undertaken by commercial organisations.

WhiteHatSecurity have a commercial scanning tool named Sentinal [24], HP have WebInspect, a commercial scanning tool [6] and IBM have the Rational AppScan suite of commercial scanning tools [7]. 

An open-source automated tool was prototyped by the Secure Systems Lab at the Technical University of Vienna - SecuBat, consisting of a multi-threaded crawling, attack and analysis components.   

By crawling 25,064 pages they were able to ascertain that 6.63\% were vulnerable to SQL Injection attacks and that 5.14% were vulnerable to XSS attacks. [9]

Other semi-automated web application scanning tools include CAL9000 [13] a browser based web application security toolkit developed by OWASP to improve certain areas of web application testing procedures. [2]

Nikto is another tool which automatically looks for pre-defined vulnerabilities in specific web applications, including XSS and SQL Injection but is unable to automatically detect unknown vulnerabilities or to look for general classes of vulnerabilities. [16] 

Nessus is a similar tool to Nikito and again can find pre-defined vulnerabilities but not general classes of vulnerabilities. [15]

Nikto and Nessus are good at finding specific exploits that have already been exposed, for example a specific SQLi vulnerability in a specific version of an open source blogging platform but neither are capable of automatically detecting XSS or SQLi vulnerabilities in general.

\subsection{Figures and Tables}
In general, figures and tables should not appear before they are cited.  Place figure captions below the figures; place table titles above the tables.  If your figure has two parts, for example, include the labels ``(a)'' and ``(b)'' as part of the artwork.  Please verify that figures and tables you mention in the text actually exist.  make sure that all tables and figures are numbered as shown in Table \ref{units} and Figure 1.
%sort out your own preferred means of inserting figures

\begin{table}[htb]
\centering
\caption{UNITS FOR MAGNETIC PROPERTIES}
\vspace*{6pt}
\label{units}
\begin{tabular}{ccc}\hline\hline
Symbol & Quantity & Conversion from Gaussian \\ \hline
\end{tabular}
\end{table}

\subsection{References}

The list of cited references should appear at the end of the report, ordered alphabetically by the surnames of the first authors.  References cited in the main text should use Harvard (author, date) format.  When citing a section in a book, please give the relevant page numbers, as in \cite[p293]{budgen}.  When citing, where there are either one or two authors, use the names, but if there are more than two, give the first one and use ``et al.'' as in  , except where this would be ambiguous, in which case use all author names.

You need to give all authors' names in each reference.  Do not use ``et al.'' unless there are more than five authors.  Papers that have not been published should be cited as ``unpublished'' \cite{euther}.  Papers that have been submitted or accepted for publication should be cited as ``submitted for publication'' as in \cite{futher} .  You can also cite using just the year when the author's name appears in the text, as in ``but according to Futher \citeyear{futher}, we \dots''.  Where an authors has more than one publication in a year, add `a', `b' etc. after the year.

\section{Solution}

\subsection{Architecture and Design}
The following component diagram provides an architectural overview of the proposed solution: Project Norris.

%insert picture/figure

Project Norris is a web-based application that allows a user to input a URL of a website, and then after crawling, parsing, and attacking the website, displays a report to the user of any security vulnerabilities found in the website by Project Norris.

The design of Project Norris has been broken down into a number of core components.  The user interface of Project Norris is handled by an MVC-based web application.  There are then two main back-end components which do the heavy processing for the application.  These are the Attacker component and the Crawler component.

The Web GUI interacts with the back-end components by placing jobs on another component, the Queue.
The Attacker and Crawler are further broken down into two components each, one being a worker script which is continuously run that checks for new jobs on the Queue and the other component being the code that is run by the script when a job is found.

Many of the components also rely on another component, the Database. This is to allow data to be stored persistently rather than only in-memory.

The Queue component relies heavily on the Database because when a job is added to the Queue it is actually inserted into the Database.  Consequently when the two worker scripts are asking the Queue if there are any available jobs the Queue actually just queries the database.

The reason to separate the back-end from the front-end of the web application is to deal with the issue of cohesion and coupling, and performance and scalability. 

\subsection{Algorithms}
There are two vulnerability detection algorithms in Project Norris.  Both are fairly primitive and follow a similar work flow:

For a given form location: send malicious data, analyse the server response or the HTTP Response Body of the generated page to ascertain whether the attack was successful.

The difference between the two algorithms is the method of generating the malicious data to send.

\subsubsection{Static Brute Forcing}
Static Brute Forcing will take a list of known attack vectors, and permeate through the list until one succeeds or none of the attacks succeed.

The list of attack vectors will be generated from the XSS Cheat Sheet [3].

The pseudo-code for the algorithm is along the following lines:

%insert pseudo-code

This algorithm is primarily used for detecting XSS vulnerabilities but an even simpler version that inputs only one single-quote - ‘ - will be used to detect SQLi vulnerabilities.

\subsubsection{Fuzzy Brute Forcing}
Fuzzy Brute Forcing dynamically generates potentially malicious input by generating a large amount of random data.

The pseudo-code for the algorithm is along the following lines:

%insert pseudo-code

This algorithm will be used to look at a different set of vulnerabilities: buffer overflow vulnerabilities and directory traversal vulnerabilities.
\section{Results}

this section presents the results of the solutions.  It should include information on experimental settings.  The results should demonstrate the claimed benefits/disadvantages of the proposed solutions.

This section should be between 2 to 3 pages in length.

\section{Evaluation}

This section should between 1 to 2 pages in length.

\section{Conclusions}

Current web application vulnerability detection techniques mostly involve static code analysis and, generally, this is limited to applications written in the PHP language.

Norris can be used to find vulnerabilities in web applications written in any language; it is a black box vulnerability scanner. 

Due to the designed architecture, Norris reports vulnerabilities as soon as it finds them, meaning it can be effectively used with web applications of any size.

It is an effective aide in the hunt for web application vulnerabilities and can crawl, parse and attack a website far quicker than a manual scan can.

\bibliography{projectpaper}

\end{document}
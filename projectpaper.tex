\documentclass[12pt,a4paper]{article}
\usepackage{times}
\usepackage{durhampaper}
\usepackage{harvard}
\usepackage{listings} 

\citationmode{abbr}
\bibliographystyle{agsm}

\title{Analysis of Web Application Vulnerability Detection Algorithms}
\author{David Budgen}
\student{A.J.C. Taylor}
\supervisor{W.W. Song}
\degree{BSc Software Engineering}

\date{}

\begin{document}

\maketitle

\begin{abstract}
These instructions give you guidelines for preparing the final paper.  DO NOT change any settings, such as margins and font sizes.  Just use this as a template and modify the contents into your final paper.  Do not cite references in the abstract.

The abstract must be a Structured Abstract with the headings {\bf Context/Background}, {\bf Aims}, {\bf Method}, {\bf Results}, and {\bf Conclusions}.  This section should not be longer than half of a page, and having no more than one or two sentences under each heading is advised.
\end{abstract}

\begin{keywords}
web application security, sql injection, cross site scripting, xss, brute force, fuzzing
\end{keywords}

\section{Introduction}
The aim of the project is to create and analyse web application vulnerability detection algorithms.  To support the evaluation and analysis of the algorithms, a proof-of-concept web-based tool will be developed that utilises the algorithms to detected vulnerabilities in web applications.

Hypothetically, if the tool were fully complete, the purpose of the tool would be to help security researchers and website owners to identify areas of their websites and web applications that are vulnerable to particular attacks so that they can mitigate the problem(s).

It is prohibitively expensive to perform a manual vulnerability analysis of a web application, however, an automated tool can rapidly analyse a web application using several different attack methodologies much faster than a human.

The Open Web Application Security Project (OWASP) maintain an annual list of the most prevalent vulnerabilities found in web applications. [18].  The two vulnerabilities the algorithms and tool will be primarily addressing are Cross Site Scripting (XSS) and Injection Flaws, particularly SQL Injection (SQLi) flaws in our work.

Almost half of all the vulnerabilities found in web applications were either XSS vulnerabilities or Injection Flaws, which is why these two vulnerabilities are the primary focus of this research. 

XSS vulnerabilities "allows an attacker to manipulate parts of a Web page... By injecting malicious scripting code" [4] which according to a report by CERT allows an attacker to "run malicious code in a users browser with the privileges of a legitimate script originating from the legitimate web server". [19]

Vulnerabilities from XSS attacks include stealing the user's cookies and hijacking their session and/or account, connecting users to malicious servers of the attacker's choice or performing other malicious operations on the user's machine under the guise of the vulnerable site. [12]

SQL injection attacks revolve around the attacker "trying to exploit the data provider at the backend...leading to information disclosure". [4]

SQL injection attacks “allow attackers to create, read, update, or delete any arbitrary data available to the application. In the worst case scenario, these flaws allow an attacker to completely compromise the application and the underlying systems, even bypassing deeply nested firewalled environments.” [18] 

\section{Related Work}
There are a number of different approaches to vulnerability discovery and prevention.  Some of the discovery work involves static analysis of source code (white box) and some of the prevention work involves input filtering techniques that can be applied to web applications.
Much of the automated vulnerability discovery work has been done by commercial companies, however, there are some free and/or open source implementations.

\subsection{Static Analysis}
Static analysis is a white box technique used to analyse source code for vulnerabilities.  The main limitation of static analysis tools is that they have to be rewritten for every different programming language and/or framework a web application can be written in, and so far, most of the work seems to have focussed only on PHP.  They also struggle to analyse dynamically included content.

Wasserman and Su worked on static analysis to detect XSS vulnerabilities using a mixture of tainted information flow and  string analysis and were able to provide effective checking algorithms. [22]

Wasserman and Su also worked on a static analysis tool to detect SQLi vulnerabilities.  They constructed a grammar based algorithm which tracks the effects of string operations and checks query construction to see whether users can directly or indirectly influence the query construction.   They then check if the users’ input is sufficiently  isolated from the query construction that it avoids SQLi vulnerabilities. [23]

Jovanovic, Kruegel and  Kirda created an open source tool - Pixy - for static analysis of PHP applications to detect XSS vulnerabilities.  The tool used a “flow sensitive, interprocedural, and context sensitive analysis for PHP” to detect vulnerabilities. [8]  Pixy is hampered by its inability to scan included PHP files, leading to the authors admitting a number of false-positives being returned by the tool.  Also Pixy does not deal with the object-orientated features of PHP, instead they assume that object member variables are safe, which may not be the case.

Huang et al. created a “lattice-based static analysis algorithm, derived from type systems and typestate”.  They also created a tool - WebSSARI - to analyse PHP source code for XSS and SQLi vulnerabilities. [5]

\subsection{Input Filtering}
Input filtering research has been undertaken by developing intermediary proxies that can filter out data before it reaches, for example, a user or a database.

Liu et al. developed  a proxy-tool, called SQLProb, to prevent SQLi attacks.  SQLProb sits between the front-end web servers and the back-end databases and uses genetic algorithms to dynamically detect SQLi attacks and block them from reaching the databases. [11]

SQLProb works in a similar manner to a web application firewall (WAF) and is quite a useful additional layer of security for a web application.  There are a number of other open-source WAFs such as: mod\_security for the Apache web server [14] and WebKnight for the IIS web server. [1]  There are also many commercial WAFs available.

Kirda et al. developed a proxy tool - Noxes - for end-users to sit between  web applications and the users’ browser to mitigate the effect of XSS vulnerabilities on end-users.  Noxes can be thought of as a personal web firewall for end-users. [10] Noxes is similar to the FireFox plugin NoScript [17] and suffers the same limitations in that it is extremely invasive to have to block or allow every single request from every new URI.  Noxes plays no role in discovering vulnerabilities in web applications, it is purely for protecting end users.

\subsection{Automated Black Box Vulnerability Discovery}
Automated black box vulnerability discovery tools look at a web application from a [malicious] user’s perspective, i.e. there is no source code analysis involved, and only look at the front-end of the web application.  Much of the research and developed tools have been undertaken by commercial organisations.

WhiteHatSecurity have a commercial scanning tool named Sentinal [24], HP have WebInspect, a commercial scanning tool [6] and IBM have the Rational AppScan suite of commercial scanning tools [7]. 

An open-source automated tool was prototyped by the Secure Systems Lab at the Technical University of Vienna - SecuBat, consisting of a multi-threaded crawling, attack and analysis components.   

By crawling 25,064 pages they were able to ascertain that 6.63\% were vulnerable to SQL Injection attacks and that 5.14\% were vulnerable to XSS attacks. [9]

Other semi-automated web application scanning tools include CAL9000 [13] a browser based web application security toolkit developed by OWASP to improve certain areas of web application testing procedures. [2]

Nikto is another tool which automatically looks for pre-defined vulnerabilities in specific web applications, including XSS and SQL Injection but is unable to automatically detect unknown vulnerabilities or to look for general classes of vulnerabilities. [16] 

Nessus is a similar tool to Nikito and again can find pre-defined vulnerabilities but not general classes of vulnerabilities. [15]

Nikto and Nessus are good at finding specific exploits that have already been exposed, for example a specific SQLi vulnerability in a specific version of an open source blogging platform but neither are capable of automatically detecting XSS or SQLi vulnerabilities in general.

\subsection{Summary}
As can be seen from the literature the current emphasis of academic research into web application vulnerability detection centres around static analysis of web application code bases.

This is impractical because web applications can be written in many different languages, using many different frameworks whereas the literature survey uncovers only PHP static analysis tools.  Furthermore, the literature clearly states that these tools are incomplete due to the dynamic nature of the PHP language and the ability to dynamically include code at run-time.

On the other hand, tools based on the idea of black box vulnerability scanning can be used on web applications implemented in any language because they pay no attention to the source code of the application.

Consequently, the research and development of a black box, automated vulnerability scanning tool appears to be the best direction to take to ensure maximum web application coverage.  It is hoped that this ability to analyse web applications written in any language will be a large improvement over other static analysis tools.

% \subsection{Figures and Tables}
% In general, figures and tables should not appear before they are cited.  Place figure captions below the figures; place table titles above the tables.  If your figure has two parts, for example, include the labels ``(a)'' and ``(b)'' as part of the artwork.  Please verify that figures and tables you mention in the text actually exist.  make sure that all tables and figures are numbered as shown in Table \ref{units} and Figure 1.
% %sort out your own preferred means of inserting figures
% 
% \begin{table}[htb]
% \centering
% \caption{UNITS FOR MAGNETIC PROPERTIES}
% \vspace*{6pt}
% \label{units}
% \begin{tabular}{ccc}\hline\hline
% Symbol & Quantity & Conversion from Gaussian \\ \hline
% \end{tabular}
% \end{table}
% 
% \subsection{References}
% 
% The list of cited references should appear at the end of the report, ordered alphabetically by the surnames of the first authors.  References cited in the main text should use Harvard (author, date) format.  When citing a section in a book, please give the relevant page numbers, as in \cite[p293]{budgen}.  When citing, where there are either one or two authors, use the names, but if there are more than two, give the first one and use ``et al.'' as in  , except where this would be ambiguous, in which case use all author names.
% 
% You need to give all authors' names in each reference.  Do not use ``et al.'' unless there are more than five authors.  Papers that have not been published should be cited as ``unpublished'' \cite{euther}.  Papers that have been submitted or accepted for publication should be cited as ``submitted for publication'' as in \cite{futher} .  You can also cite using just the year when the author's name appears in the text, as in ``but according to Futher \citeyear{futher}, we \dots''.  Where an authors has more than one publication in a year, add `a', `b' etc. after the year.

\section{Solution}

\subsection{Architecture and Design}

\subsubsection{Overview}
The following component diagram provides an architectural overview of the proposed solution: Project Norris.

%insert picture/figure

Project Norris is a web-based application that allows a user to input a URL of a website, and then after crawling, parsing, and attacking the website, displays a report to the user of any security vulnerabilities found in the website by Project Norris.

The design of Project Norris has been broken down into a number of core components.  The user interface of Project Norris is handled by an MVC-based web application.  There are then two main back-end components which do the heavy processing for the application.  These are the Attacker component and the Crawler component.

The Web GUI interacts with the back-end components by placing jobs on another component, the Queue.
The Attacker and Crawler are further broken down into two components each, one being a worker script which is continuously run that checks for new jobs on the Queue and the other component being the code that is run by the script when a job is found.

Many of the components also rely on another component, the Database. This is to allow data to be stored persistently rather than only in-memory.

The Queue component relies heavily on the Database because when a job is added to the Queue it is actually inserted into the Database.  Consequently when the two worker scripts are asking the Queue if there are any available jobs the Queue actually just queries the database.

The reason to separate the back-end from the front-end of the web application is to deal with the issue of cohesion and coupling, and performance and scalability. 

\subsubsection{Web GUI}
The Web GUI, or the front-end of the Project Norris web application, is designed using the MVC architectural pattern.

The various different views of the Web GUI are displayed to the users by the controllers retrieving any required data from any relevant models and then loading the relevant view and passing the data.

URLs map to controllers and methods in controllers in the following manner:

http://baseurl.com/controller/method/param

So http://baseurl.com/websites/view/1 would map to the controller Websites and the method view() passing in an ID of 1.  The page rendered would be dependent on the template loaded by the view() method.

Views are simply template files, possibly with place holders for data and simple logic for dynamic code, rendered inside a global template file, with data being passed in via controllers.

\subsubsection{Queue}
The Queue is simply a FIFO, database-backed job allocation system.  It works in the following way:

A job is inserted into the Queue (and hence the database) containing, among other things, a reference to the function of work this particular job is associated with and any arguments the job requires.

For example, to add a new website to be crawled, a job is added to the queue containing a reference to the fact that it is a crawling job and the arguments passed are the url of the website.

Worker scripts are started, and left continuously running, that check the queue at set intervals looking for work they can perform.  If they find any work, they dequeue the job and then using the module with the actually job functionality implemented they perform the task and then go back to looking at the queue until another job appears which they can perform.

The following activity diagram represents this process:

%insert activity diagram

\subsubsection{Database}
The Database is an integral component in allowing persistent storage of data and is crucial to the function of the Queue. 

The models in the Web GUI are backed by tables in the database, with an ORM layer between, allowing objects in the Web GUI to be persistently stored beyond one HTTP request.

\subsubsection{Crawler/Parser}
The Crawler receives a URL of a website that requires crawling and parsing, and then creates it’s own queue of URLs that need crawling.  

It dequeues one URL at a time and then requests the content of that page and looks for all links that do not leave the base url and that it hasn’t already seen and adds them to its queue.

The Crawler’s queue is an array of hashes of the form { url => count }.  This way it can check if it has already seen a URL by checking if url{‘count’}++ == TRUE.  Everytime it finds a URL it either inserts 

It also parses the page looking for any forms and if it finds them it adds them to the Queue for the Attacker.

The following activity diagram shows how the crawler works:

%insert activity diagram

\subsubsection{Attacker}
The attacker receives a form object and a url to attack.  It inserts the location of the form into the Database as a PointOfInterest.  

It then proceeds to attack the form, looking for Cross Site Scripting and SQL Injection vulnerabilities, by passing in data generated by two algorithms described in section 3.5.  It analyses the server response, either looking for HTTP Repsonse Codes indicating errors or by looking through the Response Body for injected code.

If any vulnerabilities are found they are inserted into the database so that reports can be generated by the Web GUI.

\subsubsection{Workflow}
The workflow  between the various components is demonstrated in the following sequence diagram:

%insert sequence diagram

Time progresses down the dashed lines.

When a user first visits Project Norris in a web browser, they are presented with an opportunity to input a URL to scan.  If they do this the Web GUI then places the URL onto the Queue.

There are two worker scripts, one for the Crawler and one for the Attacker, which are continually checking the Queue for jobs.

As soon as the Crawler sees that there is a job available (i.e. the URL just placed onto the Queue by the Web GUI) a cycle kicks in where the Crawler takes the URL and crawls the whole site finding every page on the domain.  It maintains it’s own queue of the URLs on the domain, to ensure it only looks processes new URLs.

When it find it requests each page, as well as finding all the links, if there are any forms on the page it inserts this as a job in the Queue for the Attacker.

When it has finished crawling and parsing the whole website it goes back to watching the Queue for new jobs.

The attacker takes a form off the Queue and then proceeds to attack the form and analyse the response from the web server.  Should a vulnerability be detected, it is then inserted into the database for the Web GUI to generate reports to the user.

The user is then able to generate reports in the Web GUI, which are created using the data in the database collected by the Attacker.

\subsubsection{Data Flow}
Data flows through Project Norris in the following way:

- A URL is inserted into the Queue and a Website object is created and stored in the database
- The Crawler dequeues the URL and proceeds to crawl and parse the whole domain.  Any Forms it finds are inserted into the Queue.
- The Attacker dequeues any Forms in the Queue and proceeds to attack the form.  The Forms themselves are inserted into the database by creating PointOfInterest objects and any successful attacks are stored as Vulnerability objects.  

So one Website can have many PointsOfInterest, and one PointOfInterest can have many Vulnerabilities.

The following diagram shows the data flow through the system:

\subsection{Algorithms}
There are two vulnerability detection algorithms in Project Norris.  Both are fairly primitive and follow a similar work flow:

For a given form location: send malicious data, analyse the server response or the HTTP Response Body of the generated page to ascertain whether the attack was successful.

The difference between the two algorithms is the method of generating the malicious data to send.

\subsubsection{Static Brute Forcing}
Static Brute Forcing will take a list of known attack vectors, and permeate through the list until one succeeds or none of the attacks succeed.  The list of attack vectors will be generated from the XSS Cheat Sheet 

The pseudo-code for the algorithm is along the following lines:

\begin{lstlisting}
def StaticBruteForce( Form )     
    <List> vectors;
    
    foreach (vector in vectors) { 
        attackForm( vector );
    }
end
\end{lstlisting}

This algorithm is primarily used for detecting XSS vulnerabilities but an even simpler version that inputs only one single-quote - ‘ - will be used to detect SQLi vulnerabilities.

\subsubsection{Fuzzy Brute Forcing}
Fuzzy Brute Forcing dynamically generates potentially malicious input by generating a large amount of random data.

The pseudo-code for the algorithm is along the following lines:

\begin{lstlisting} 
def FuzzyBruteForce( Form )
	for (1..1000) { 		
	    attackForm( fuzzyVector() ); 	
	}
end
\end{lstlisting}
This algorithm will be used to look at a different set of vulnerabilities: buffer overflow vulnerabilities and directory traversal vulnerabilities.

\subsubsection{Limitations}
Project Norris is a web application and this may limit it’s usefulness to the average end user, having said that, it has been written as a “hosted solution”.

Project Norris also suffers a limitation of not looking at every aspect of a website that may be vulnerable.  The focus is completely on input forms, whereas cookies and query strings may also contain vulnerabilities.

The detection of SQL Injection attacks is also limited by the fact that different web and SQL servers throw different errors on receipt of invalid SQL statements.  This means that the Attacker will have to be “trained” to understand what types of responses likely indicate an SQL Injection vulnerability, however this is likely to be imperfect.

\section{Implementation}
The crawler has been developed using a Breadth First Search algorithm.  It is passed a base URL and it then gets all the links on that page and adds them to a queue.  It then proceeds to dequeue one URL at a time and and get all the links on that page and then put them on the queue (unless any of them have already been visited - it maintains a hash table of URL => no. of times seen so it can quickly check whether or not a URL has been seen before).

Parsing is done by using a mixture of Regular Expressions and the Perl web browsing module: WWW::Mechanize.

The attacking component is passed an object representing a form (HTML::Form) from the crawler, and then proceeds to input different attack vectors, submit the form and parse the HTTP Response Body or Header, sent from the server, to see whether or not the attack was successful.

The attacking component is also responsible for storing data.  Every time it starts a job it adds information about the job into the database so in the GUI every point of possible vulnerability (PointOfInterest) is reportable. 

For any successful attacks a vulnerability is stored which is associated with a specific PointOfInterest (which is in turn associated with a Website).

The database for storing data related to a website is as follows:

A \em{Website} contains a name and URL.
A \em{Website} has many \em{PointsOfInterest} where a point of interest is a URL.
A \em{PointOfInterest} has many \em{Vulnerabilities} where a \em{Vulnerability} has a type and an input string.

Two intermediary tables are used to normalise the many-to-many relationships: \em{WebsitePointOfInterest} and \em{PointOfInterestVulnerability}.

The GUI is an simple web application that allows a user to add or delete websites, to initiate a new scan of a website (which means to place the URL onto the job queue for the crawler and attacker).

The job queue uses another database which contains a table of jobs.  Worker scripts watch the database and pull off jobs if they are capable of performing the work.

\subsection{Web GUI Implementation}
The GUI has been developed using the Catalyst web framework.  Catalyst is a Perl model-view-controller (MVC) based framework.  It provides helpers for many common tasks in web application development, provides architectural guidance yet still allows for flexibility to develop the application as desired.

DBIx::Class is a Perl object relational mapper (ORM).  It is used to map objects created in the GUI directly to rows in the database.  It provides a clean interface for constructing SQL queries and persistently storing objects and has been used alongside Catalyst in the development of the GUI.

\subsection{Job Queue}
The Job Queue is used to pass jobs between the GUI, the Crawler and the Attacker.  Without the Job Queue this whole communication process would be tightly coupled and, save multi-threading, would cause the GUI to lock up while comments finished their tasks.

The Queue is implemented using a Perl module called TheSchwartz, which uses the Database (MySQL 5) for storing specific jobs as well as meta data about which components can perform which tasks.

The Crawler and the Attacker and then started up and continuously poll the database/queue looking for new jobs which they can perform, once they are complete they then look for more jobs.

\subsection{Crawler}
The Crawler receives a URL from the GUI in the job it receives from the Queue.  It then initialises a queue of its own with the initial URL on and enters a while loop, whose precondition is that the queue has at least one item on it.  The Crawler then proceeds to find every link on that page using the Perl module WWW::Mechanize.  As well as the queue it maintains a hash of { URL => no. of times seen }.  For each link it finds on each page it checks whether or not it exists in the hash, if not it adds that link to the queue and if so it simply increments the count of the number of times it has been seen.

It also performs some additional checks before following a URL: it makes sure that it only follows links to pages that are of content type 'text/html',  it only follows URLs of pages on the same top level domain (TLD) as the initial URL and it treats http://example.com/about and http://example.com/about#careers as the same URL to avoid excessively crawling the same page.

While on a page it also looks for PointsOfInterest that can be passed to the Attacker.  If it finds a form on the page, it inserts a new job on the Queue with a reference to the form it found for the Attacker to perform XSS and SQLi attacks.  If it finds a URI fragment, such as, http://example.com/index.php?page=../home.php, it inserts a new job on the Queue with a reference to the URI for the Attacker to perform a directory traversal attack.

\subsection{Attacker}
The Attacker component first checks to see whether it has received a URI or a Form PointOfInterest and stores the PointOfInterest in the Database for the GUI.  If it is a URI PointOfInterest it executes the directory traversal, fuzzy-based algorithm.  This works by attempting to access <\$input\_uri>../etc/passwd, then <\$input\_uri>../../etc/passwd and so on and looking, via regular expression, for the word 'root' (as in the root user accout) in the HTTP Response Body returned from the server.  If it is found a Vulnerability is stored in the database for that particular PointOfInterest.

This can return false positives and only works on unix based servers.

If it has been passed a Form PointofInterest from the Crawler it first executes the XSS algorithm and then the SQLi algorithm.  The XSS algorithm works by permiating through a list of predefined attack vectors. For iteration through the list of vectors the vector is set as the input for each field of the form, which is then submitted, and in the HTTP Response Body the attack vector is searched for, via regular expressions.  If it is found then it is assumed that the vector was successful and a Vulnerability is stored in the database for that PointOfInterest.

The SQLi injection algorithm works by inserting a single quote - ' - into all the input fields and attempts to work out if a server or database error was thrown in response.  This is done by checking the HTTP Response Header from the server to check whether or not it is in the 50x range - indicating an error - or looking for common database error phrases output by different database systems in the HTTP Response Body.  

This is incomplete as sometimes false positives are found if certain phrases assumed to demonstrate SQLi vulnerabilities are present in the HTTP Response Body and also we do not have a full list of all the errors sent by all different types and versions of database systems.
\section{Results}

this section presents the results of the solutions.  It should include information on experimental settings.  The results should demonstrate the claimed benefits/disadvantages of the proposed solutions.

This section should be between 2 to 3 pages in length.

\section{Evaluation}

This section should between 1 to 2 pages in length.

\section{Conclusions}

Current web application vulnerability detection techniques mostly involve static code analysis and, generally, this is limited to applications written in the PHP language.

Norris can be used to find vulnerabilities in web applications written in any language; it is a black box vulnerability scanner. 

Due to the designed architecture, Norris reports vulnerabilities as soon as it finds them, meaning it can be effectively used with web applications of any size.

It is an effective aide in the hunt for web application vulnerabilities and can crawl, parse and attack a website far quicker than a manual scan can.

\bibliography{projectpaper}

\end{document}